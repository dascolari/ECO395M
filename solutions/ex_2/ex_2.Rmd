---
title: "Exercise 2"
author: "David Scolari"
date: "3/7/2022"
output: md_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
```{r source, include=FALSE}
library(here)
path <- here()
source(file.path(path, "code", "lib.R"))
source(file.path(path, "code", "ex_2_1.R"))
source(file.path(path, "code", "ex_2_2.R"))
source(file.path(path, "code", "ex_2_3.R"))
source(file.path(path, "code", "ex_2_4.R"))
```

# Problem 1

```{r boarding hour}
boardings_hour
```

The above figure shows average hourly boardings for UT Cap Metro rides over a three month period, faceted by day of the week. Daily trends look similar accross all weekdays, with a peak around 4-5 PM. However, weekend boardings are much lower than weekdays and do not have a sharp peak. Boardings on Mondays in September look lower compared to other days and months because Labor Day is a day off from school, so there is one less Monday in September where students are on campus. Similarly, boardings on Weds/Thurs/Fri in November look lower likely due to the Thanksgiving holiday.

```{r boarding temp}
boardings_temp
```

In the above figure, each facet plots UT CapMetro boardings by temperature on weekdays and weekends. Holding hour of day and weekend status constant, temperature does NOT seem to have a noticeable effect on the number of UT students riding the bus. This is seen by the relatively flat trends across all hours. A very slight upward trend might be present in the early morning hours as well as the late night hours, but it is very slight and likely insignificant. Demand for the bus on UT does not appear to respond to changes in temperature. 

# Problem 2

For this exercise, we try to predict Saratoga housing prices using a set of features. I built two models to outperform the "medium" linear model, one linear model and one KNN model.

## Linear Model

The first model is another linear model that I call "lm step". This model starts with lm medium as a baseline and then uses step wise selection to choose additional covariates as well as a slate of double interactions to add as features to the model. The table below shows that the cross validated rmse of lm_step is about 1000 less than the cross validated rmse of the medium linear model.

## KNN Model

The second model that I built is a cross-validated k-nearest-neighbors model. I chose to use all of the x covariates as features because doing so seemed to have the best model performance. The cross validated rmse for the KNN model is lower than both linear models by a fair amount, 4-5 thousand. 

In this case study, we see that the KNN model outperforms both linear models. Moreover, the gains from feature engineering with the linear models are very modest. The stepwise selection specification is what I landed on after trying several other options, none of which improved out of sample performance over the medium model and some of which actually performed worse. However, just by switching to KNN, even a naivly chosen feature set (just all of them thrown in) improves the model's out of sample performance by a few thousand in rmse. This is likely due to a non-linear relationship between Saratoga housing prices and the set of covariates; the linear models just don't do well at capturing that relationship.

```{r saratoga houses}
fit
```

# Problem 3

```{r default bar}
default_bar
```

In the above bar chart, we can see that credit history predicts a high default probability for those with good credit history and the lower probability to those with a bad or terrible history. This is likely a result of the bank's sampling of loans that defaulted for this data set. Many of those defaulted loans may have been from people with good credit scores, but that is probably because most people in general have a good credit history. We are interested in probability of default in the entire population, not just those who defaulted.

In the below confusion matrix and rate matrix, we see that the model has a low true positive rate. This indicates that among actual defaults, the model is not good at correctly classifying them as such. This is probably due to the fact that it is only looking at data sampled from actual defaulting loans.

Because of the bad sampling practice that biases the data, this is not a good data set to classify prospective borrowers as "high" versus "low" probability of default. The bank should sample from all loans, not just those which default.

```{r default confuso}
confusion_default_logit
rate_matrix
```

# Problem 4

## Model Building

```{r hotels perform}
performance_matrix
```

The above table assesses the out of sample performance of my "best" linear predictor model with the two baselines, "small" and "big" with a cross-validated confusion matrix. Of note is the high TPR and lower FPR and FDR for the best model as compared to the big and small models.

My strategy for building the best model was to create useful interactions that do a good job of separating out the types of reservations and characteristics of a party with children. For example, parties with children are more probably more common in the summer months due to kids being out of school. Similarly, groups of kids probably travel with their two parents, so the number of adults is important. Using this kind of logic, I chose a slate of interactions that does a much better job than the big and small models. 

## Model Validation: Step 1

After building the models, the next step is to validate it on new data. The ROC curve shows the relationship between true and false positive rates for different decision parameters from testing the models on the validation data set. The best model concaves up and to the right more than the big model. This is an indication of better out of sample model performance. 

```{r hotels roc}
roc_curve
```

## Model Validation: Step 2

For the final validation step, I split the validation set 20-fold to simulate trials of a busy weekend of reservations. The below tables and bar charts show the predicted, expected, and observed total children from those trials.

One thing to notice in these results is the behavior of predicted and expected children relative to the actual total. Using the ROC curve, I chose a decision parameter of 0.16, which strikes a balance between a high TPR and low FPR. Because of this, the model's prediction rarely (never for this particular split of the data) underestimates the number of children in a trial, while the expected children centers around the actual number, sometimes bigger and sometimes smaller. 
It might be desirable for a hotel to rarely or never underestimate the number of kids it will host on a given weekend because it is bad marketing to run out of amenities promised to guest, even if it means some of the supply goes unused. The best model here does a good job of that. 

```{r hotels 20fold}
hotels_20fold_bar
hotels_20fold
```






